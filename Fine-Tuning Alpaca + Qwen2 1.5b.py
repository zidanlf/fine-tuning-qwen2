# -*- coding: utf-8 -*-
"""Fine-Tuning Alpaca + Qwen2 1.5b .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GCGJlMOaHo7ZmIfuX4IQHF2HQKWNHASQ
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048
dtype = None
load_in_4bit = True .

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen2-1.5b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

from transformers import AutoTokenizer
import pandas as pd

def generate_output(input_text):
    # Gabungkan teks masukan
    prompt = f"Input:\n{input_text}\n\nResponse:\n"

    # Encode the prompt text
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)

    # Gunakan model untuk menghasilkan teks
    generated_ids = model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_length=512, do_sample=True, temperature=0.7)

    # Decode the generated ids to text
    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    return output_text

import pandas as pd

# Membaca file Parquet
df_train =pd.read_parquet('/content/train-00000-of-00001.parquet')
df_test = pd.read_parquet('/content/test-00000-of-00001.parquet')
df_val = pd.read_parquet('/content/validation-00000-of-00001.parquet')
df_prompt = pd.read_parquet('/content/prompt-00000-of-00001.parquet')

df_combined = pd.concat([df_val, df_prompt, df_train, df_test])

# Menyimpan DataFrame menjadi file Parquet
df_combined.to_parquet('/content/all_combined_file.parquet')

def generate_output_from_parquet(parquet_file):
    df = pd.read_parquet(parquet_file)
    prompts = df['prompt']
    collected_data = {"Entry": [], "Input": [], "Response": []}

    for idx, prompt in enumerate(prompts, start=1):
        output_text = generate_output(prompt)
        collected_data["Entry"].append(idx)
        collected_data["Input"].append(prompt)
        collected_data["Response"].append(output_text)

    return collected_data

# Fungsi untuk menyimpan ke Excel
def save_to_excel(data, output_file):
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)
    print(f"Data telah disimpan ke {output_file}")

collected_data = generate_output_from_parquet('/content/all_combined_file.parquet')

# Menyimpan data ke Excel
save_to_excel(collected_data, "all_before1.xlsx")

from google.colab import files

files.download("all_before1.xlsx")

"""<a name="Data"></a>
### Data Prep
We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.

**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).

**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!

If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).

For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing).
"""

alpaca_prompt = """{}

### Instruction:
{}

### Output:
{}"""

# Buat fungsi untuk memformat prompt
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    outputs = examples["output"]
    texts = []
    for instruction, output in zip(instructions, outputs):
        text = alpaca_prompt.format(instruction, "", output) # Hanya menggunakan token EOS di akhir prompt
        texts.append(text)
    return { "text" : texts }

# Load dataset
from datasets import load_dataset
dataset = load_dataset("mlabonne/Evol-Instruct-Python-26k", split="train")
dataset = dataset.map(formatting_prompts_func, batched=True)

"""<a name="Train"></a>
### Train the model
Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!
"""

from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=30,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
        save_strategy="steps",
        save_steps=25,
    ),
)

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 8,

        # Use num_train_epochs = 1, warmup_ratio for full training runs!
        warmup_steps = 20,
        max_steps = 120,

        learning_rate = 5e-5,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        save_strategy="steps",
        save_steps=25,
    ),
)

#@title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

"""<a name="Inference"></a>
### Inference
Let's run the model! You can change the instruction and input - leave the output blank!
"""

from transformers import TextStreamer
import pandas as pd

def generate_output_with_instruction(instruction):
    alpaca_prompt = "write code to python based on input.\n\nInput:\n{}\n\nResponse:\n{}"

    FastLanguageModel.for_inference(model)

    # Buat inputs menggunakan tokenizer
    inputs = tokenizer(
        [
            alpaca_prompt.format(
                instruction,  # instruction
                "",  # response - kosongkan untuk generasi!
            )
        ],
        return_tensors="pt"
    ).to("cuda")

    # Inisialisasi text streamer
    text_streamer = TextStreamer(tokenizer)

    # Generate output dengan model
    output = model.generate(
        **inputs,
        streamer=text_streamer,
        max_new_tokens=256,
        max_length=512  # Menambahkan maksimum panjang output
    )

    return tokenizer.decode(output[0], skip_special_tokens=True)

from google.colab import files

def generate_output_from_parquet(prompt):
    output_text = generate_output_with_instruction(prompt)
    return output_text

# Fungsi untuk menyimpan ke Excel
def save_to_excel(data, output_file):
    df = pd.DataFrame(data, columns=["Entry", "Input", "Response"])
    df.to_excel(output_file, index=False)
    print(f"Data telah disimpan ke {output_file}")

# Fungsi untuk mengumpulkan output dan input dari setiap prompt
def collect_output_with_instruction_from_parquet(parquet_file):
    df = pd.read_parquet(parquet_file)
    prompts = df['prompt']
    collected_data = {"Entry": [], "Input": [], "Response": []}

    for idx, prompt in enumerate(prompts, start=1):
        print(f"Entry {idx}:")
        output_text = generate_output_from_parquet(prompt)
        collected_data["Entry"].append(idx)
        collected_data["Input"].append(prompt)
        collected_data["Response"].append(output_text)

    return collected_data


collected_data = collect_output_with_instruction_from_parquet('/content/all_combined_file.parquet')

save_to_excel(collected_data, "all_after1.xlsx")

files.download("all_after1.xlsx")

save_to_excel(collected_data, "pvt_after1.xlsx")

files.download("pvt_after1.xlsx")

import pandas as pd
import re

# Fungsi untuk mengambil kode Python dari respons
def extract_python_code(response):
    # Cari bagian yang diawali dengan ```python dan diakhiri dengan ```
    matches = re.findall(r'```python\n(.*?)\n```', response, re.DOTALL)
    if matches:
        return matches[0].strip()
    else:
        return None

# Baca file Excel yang berisi respons
df = pd.read_excel("/content/output_valPrompt_before.xlsx")

# Inisialisasi list untuk menyimpan kode Python
python_codes = []

# Iterasi melalui setiap respons
for response in df['Response']:
    # Ekstrak kode Python dari respons
    python_code = extract_python_code(response)
    if python_code:
        python_codes.append(python_code)

# Buat DataFrame baru dari kode Python
df_python = pd.DataFrame({"Source Code Python": python_codes})

# Simpan DataFrame ke dalam file Excel
df_python.to_excel("source_code_python_valPrompt_before.xlsx", index=False)

print("Source code Python telah disimpan dalam file Excel.")

files.download("source_code_python_valPrompt_before.xlsx")

# Baca file Excel yang berisi respons
df = pd.read_excel("/content/output_valPrompt_after.xlsx")

# Inisialisasi list untuk menyimpan kode Python
python_codes = []

# Iterasi melalui setiap respons
for response in df['Response']:
    # Ekstrak kode Python dari respons
    python_code = extract_python_code(response)
    if python_code:
        python_codes.append(python_code)

# Buat DataFrame baru dari kode Python
df_python = pd.DataFrame({"Source Code Python": python_codes})

# Simpan DataFrame ke dalam file Excel
df_python.to_excel("source_code_python_valPrompt_after.xlsx", index=False)

print("Source code Python telah disimpan dalam file Excel.")

files.download("source_code_python_valPrompt_after.xlsx")